---
title: "Practical Machine Learning"
author: "Oscar Legat"
date: "31 de enero de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the necessary libraries

These are the libraries needed to execute correctly the Rmd Script.


```{r}
library(caret)
library(randomForest)
library(caret)
library(e1071)
```


## Loading data

```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

valUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

data <- read.csv(url(trainUrl), header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))

validation <- read.csv(url(valUrl), header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))

```

## Removing some features

It has been deducted that the following features have to be removed, because they have information related to order of the registers, the user who get  the data, and information about the timestamp. 


```{r}
data$X <- NULL
data$user_name <- NULL
data$raw_timestamp_part_1 <- NULL
data$raw_timestamp_part_2 <- NULL
data$cvtd_timestamp <- NULL
data$new_window <- NULL
data$num_window <- NULL

validation$X <- NULL
validation$user_name <- NULL
validation$raw_timestamp_part_1 <- NULL
validation$raw_timestamp_part_2 <- NULL
validation$cvtd_timestamp <- NULL
validation$new_window <- NULL
validation$num_window <- NULL

```


## Detecting missing data and treatment

To treat missing data, first try to detect the NAs and the proportions of those. We will considerer a not usefull feature, if the feature's percentage of Nas is greater than 70%.
Relying on this, all features with NAs have more than 70% of NAs, then we remove these features, in training data, and also in validation data.

```{r}

propmiss <- function(dataframe) {
  m <- sapply(dataframe, function(x) {
    data.frame(
      nmiss=sum(is.na(x)), 
      n=length(x), 
      propmiss=sum(is.na(x))/length(x)
    )
  })
  d <- data.frame(t(m))
  d <- sapply(d, unlist)
  d <- as.data.frame(d)
  d$variable <- row.names(d)
  row.names(d) <- NULL
  d <- cbind(d[ncol(d)],d[-ncol(d)])
  return(d[order(d$propmiss), ])
}

missData <- propmiss(data)

missData

missDataVars <- missData[missData$propmiss > 0.70,]$variable
data <- data[, !names(data)%in%missDataVars]
validation <- validation[, !names(validation)%in%missDataVars]

```

## Detecting strong correlations and treatment

We are insterested in remove those variables that are strongly correlated. We consider strong correlation if it is equal or greater than 0.9 .

```{r}


last <- as.numeric(ncol(data))
penultimate <- last - 1

for (i in 1:penultimate) {
  data[,i] <- as.numeric(data[,i])
  validation[,i] <- as.numeric(validation[,i])
}

strong_correlations <- findCorrelation(cor(data[, -c(last)]), cutoff=0.9)

data <- data[, -strong_correlations]
validation <- validation[, -strong_correlations]

```

##Before to train the model, preprocessing features

To normalize the features, it's used knnImpute, center and scale. The normalization usually helps to the model to distinguess better how to select the most important features to get the different decissions to clasification. 

```{r}

last <- as.numeric(ncol(data))

knn_center_scale <-preProcess(data[,-c(last)],method=c('knnImpute', 'center', 'scale'))
dataPreprocesed <- predict(knn_center_scale, data[,-c(last)])
dataPreprocesed$classe <- data$classe

valPreprocesed <-predict(knn_center_scale,validation[,-c(last)])
valPreprocesed$problem_id <- validation$problem_id

```


### Train the model

We create the partitions first, to obtain from the data set the training and testing set. After that
the Random Forest Model is trained based on the training set.

```{r}

inTrain <- createDataPartition(y=dataPreprocesed$classe, p=0.7, list=FALSE )
training <- dataPreprocesed[inTrain,]
testing <- dataPreprocesed[-inTrain,]


# set seed
set.seed(12345)


last <- as.numeric(ncol(training))

# get the best mtry
bestmtry <- tuneRF(training[,-c(last)],training$classe, ntreeTry=100, 
                   stepFactor=1.5, trace=TRUE, plot=TRUE, dobest=FALSE)

mtry <- bestmtry[as.numeric(which.min(bestmtry[,"OOBError"])),"mtry"]

#Random Forest training
rdf <-randomForest(classe~.,data=training, mtry=mtry, ntree=501, 
                      keep.forest=TRUE, proximity=TRUE, 
                      importance=TRUE,test=testing)

```


## Acuraccy results

We are trying to see, how accuraccy the model is. We realize that although the training has am accuraccy of 1, acuraccy of the testing sample is 0.99. It seems that there is no overfitting, and it fits well.


```{r}

pred_training <- predict(rdf, newdata = training)
postResample(pred_training, training$classe)
pred_testing <- predict(rdf, newdata = testing)
postResample(pred_testing, testing$classe)

```


## Predictions for results

Finally we use the validation of twenty registers to predict the results.

```{r}
results<-predict(rdf,valPreprocesed)
results
```

